{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySDS Week 03 Day 03 v.1 - Exercise - Webcrawlers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying a scraper to suit needs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1. \n",
    "# How pervaisive is the notion of a social network at the OII? \n",
    "# Modify the crawler that we showed in class. You can either use a new scraper \n",
    "# from scrapy, beautifulSoup, mechanicalSoup or modify the code we have. \n",
    "#\n",
    "# Part 1. Write pseudocode from the following instructions:\n",
    "#\n",
    "# Use the department's homepage (http://www.oii.ox.ac.uk) as your seed. \n",
    "# Navigate to all links that you find that also have www.oii.ox.ac.uk in them.\n",
    "# If a page includes the word \"network\" or \"networks\", then mark it in the \n",
    "# \"has network\" pile. Otherwise mark it in the \"not mentioned\" pile. \n",
    "# When you run out of links, return the number in each pile. \n",
    "#\n",
    "# NOTE> Please exempt any page with http://www.oii.ox.ac.uk/study See updated code snippet. \n",
    "\n",
    "################################\n",
    "# Answer below here \n",
    "\n",
    "\n",
    "'''\n",
    "Take seed pages and add to set of pages to visit\n",
    "\n",
    "Repeat the following using while loop until out of pages or user defined page limit exceeded:\n",
    "\n",
    "    Parse next webpage in the set of pages to visit,\n",
    "    returning the page data and all links on the webpage that are within oii (but not study section)\n",
    "\n",
    "    for all returned links\n",
    "        if the link is not in our set of all pages visited\n",
    "            add it to the set of pages to visit\n",
    "\n",
    "    if any of the stopwords are in the page data\n",
    "        add the url to the pages with words list\n",
    "    else\n",
    "        add the url to the pages without words list\n",
    "        \n",
    "    sleep for defined period, and increment page count\n",
    "\n",
    "return lists of pages with the words, and pages without the words\n",
    "'''\n",
    "\n",
    "\n",
    "################################\n",
    "# Peer review comments below here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2. Creating a scraper that looks for oii links. \n",
    "\n",
    "# If you are using a subclass of HTTPparser, consider reviewing the documentation \n",
    "# https://docs.python.org/3/library/html.parser.html\n",
    "#\n",
    "# Write a subclass of HTTPparser (or another means) \n",
    "# that returns links if they have 'www.oii.ox.ac.uk'\n",
    "# in the full path. You should think about how you are going to test this.\n",
    "\n",
    "################################\n",
    "# Answer below here \n",
    "\n",
    "from html.parser import HTMLParser \n",
    "import urllib.error\n",
    "from urllib.request import urlopen  \n",
    "from urllib import parse\n",
    "import time\n",
    "\n",
    "#Â Very much based on the parser / spider commands from the lecture\n",
    "\n",
    "# We are going to create a class called LinkParser that inherits some\n",
    "# methods from HTMLParser which is why it is passed into the definition\n",
    "class LinkParser(HTMLParser):\n",
    "\n",
    "    # This is a function that HTMLParser normally has\n",
    "    # but we are adding some functionality to it\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # We are looking for the begining of a link. Links normally look\n",
    "        # like <a href=\"www.someurl.com\"></a>\n",
    "        if tag == 'a':\n",
    "            for (key, value) in attrs:\n",
    "                if key == 'href':\n",
    "                    # We are grabbing the new URL. We are also adding the\n",
    "                    # base URL to it. For example:\n",
    "                    # www.saintlad.com is the base and\n",
    "                    # somepage.html is the new URL (a relative URL)\n",
    "                    #\n",
    "                    # We combine a relative URL with the base URL to create\n",
    "                    # an absolute URL like:\n",
    "                    # www.saintlad.com/somepage.html\n",
    "                    newUrl = parse.urljoin(self.baseUrl, value)\n",
    "                    # And add it to our colection of links:\n",
    "                    if 'www.oii.ox.ac.uk' in newUrl[:25] and 'www.oii.ox.ac.uk/study' not in newUrl: #### check if link is within oii but not in 'study' section\n",
    "                        self.links = self.links + [newUrl]\n",
    "\n",
    "    # This is a new function that we are creating to get links\n",
    "    # that our spider() function will call\n",
    "    def getLinks(self, url):\n",
    "        self.links = []\n",
    "        # Remember the base URL which will be important when creating\n",
    "        # absolute URLs\n",
    "        self.baseUrl = url\n",
    "        # Use the urlopen function from the standard Python 3 library\n",
    "        response = urlopen(url)\n",
    "        # Make sure that we are looking at HTML and not other things that\n",
    "        # are floating around on the internet (such as\n",
    "        # JavaScript files, CSS, or .PDFs for example)\n",
    "        # BH: I changed this to text/html in rather than == text/html, since \n",
    "        #     some pages have text/html; encoding=utf-8.\n",
    "        if 'text/html' in response.getheader('Content-Type'):\n",
    "            htmlBytes = response.read()\n",
    "            # Note that feed() handles Strings well, but not bytes\n",
    "            # (A change from Python 2.x to Python 3.x)\n",
    "            htmlString = htmlBytes.decode(\"utf-8\")\n",
    "            self.feed(htmlString)\n",
    "\n",
    "            return htmlString, self.links\n",
    "        else:\n",
    "            return \"\",[]\n",
    "        \n",
    "def spider(seed_set,stop_words,max_pages,sleep=0.2): \n",
    "    page_count = 0\n",
    "    pages_with_words = []\n",
    "    pages_without_words = []\n",
    "    all_pages = set(seed_set)\n",
    "    \n",
    "    try: \n",
    "        pages_to_visit  = list(all_pages)\n",
    "    except: \n",
    "        print(\"Spider expects a collection of URLs as first argument.\")\n",
    "\n",
    "    parser = LinkParser()\n",
    "    \n",
    "    while len(pages_to_visit) and page_count < max_pages:\n",
    "        url = pages_to_visit[0]\n",
    "        print(url) # progress indicator\n",
    "        pages_to_visit = pages_to_visit[1:] #Get rid of first page        \n",
    "        page_count += 1\n",
    "        try:\n",
    "            data, links = parser.getLinks(url) # get link data   \n",
    "        except urllib.error.HTTPError: # catch 404 errors\n",
    "            print('#############404 Error################', url)\n",
    "            continue\n",
    "            \n",
    "        for i in links: # add links to all pages / pages to visit if not already\n",
    "            if i not in all_pages:\n",
    "                all_pages.add(i)\n",
    "                pages_to_visit.append(i)\n",
    "\n",
    "        if any(word in data for word in stop_words): # check if any of the stop words are in the text\n",
    "            pages_with_words.append(url)            \n",
    "        else: \n",
    "            pages_without_words.append(url)\n",
    "\n",
    "        time.sleep(sleep)\n",
    "    return (pages_with_words,pages_without_words)\n",
    "\n",
    "################################\n",
    "# Peer review comments below here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.oii.ox.ac.uk/\n",
      "https://www.oii.ox.ac.uk/research/\n",
      "https://www.oii.ox.ac.uk/research/projects/\n",
      "https://www.oii.ox.ac.uk/research/publications/\n",
      "https://www.oii.ox.ac.uk/research/policy/\n",
      "https://www.oii.ox.ac.uk/research/ref/\n",
      "https://www.oii.ox.ac.uk/people/\n",
      "https://www.oii.ox.ac.uk/people/new-positions/\n",
      "https://www.oii.ox.ac.uk/events/\n",
      "https://www.oii.ox.ac.uk/events/series/\n",
      "https://www.oii.ox.ac.uk/events/past-events/\n",
      "https://www.oii.ox.ac.uk/videos/\n",
      "https://www.oii.ox.ac.uk/videos/playlists/\n",
      "https://www.oii.ox.ac.uk/news/\n",
      "https://www.oii.ox.ac.uk/news/releases/\n",
      "https://www.oii.ox.ac.uk/news/coverage/\n",
      "https://www.oii.ox.ac.uk/about/\n",
      "https://www.oii.ox.ac.uk/about/giving/\n",
      "https://www.oii.ox.ac.uk/oxford-internet-institute-awards/\n",
      "https://www.oii.ox.ac.uk/about/library/\n",
      "https://www.oii.ox.ac.uk/about/find-us/\n",
      "https://www.oii.ox.ac.uk/blog/\n",
      "https://www.oii.ox.ac.uk/follow-us/\n",
      "https://www.oii.ox.ac.uk/alumni/\n",
      "https://www.oii.ox.ac.uk/news/releases/junk-news-dominating-coverage-of-us-midterms-on-social-media-new-research-finds/\n",
      "https://www.oii.ox.ac.uk/news/releases/screen-time-does-not-disrupt-childrens-sleep-new-study-finds/\n",
      "https://www.oii.ox.ac.uk/videos/oii-london-lecture-show-me-your-data-and-ill-tell-you-who-you-are/\n",
      "https://www.oii.ox.ac.uk/blog/two-new-senior-research-fellows-to-join-the-oxford-internet-institute/\n",
      "https://www.oii.ox.ac.uk/events\n",
      "https://www.oii.ox.ac.uk/events/charting-connections-in-social-media/\n",
      "https://www.oii.ox.ac.uk/events/surveillance-capitalism-governance-and-social-justice-moving-beyond-data-centrism/\n",
      "https://www.oii.ox.ac.uk/events/oxdeg-answering-the-call-of-duty-popular-geopolitics-and-playing-virtual-war/\n",
      "https://www.oii.ox.ac.uk/events/from-identity-and-inequality-to-aggregation-and-transfer-discourses-of-social-differentiation-in-a-smart-city/\n",
      "https://www.oii.ox.ac.uk/research\n",
      "https://www.oii.ox.ac.uk/research/projects/transnet/\n",
      "https://www.oii.ox.ac.uk/research/projects/crowdlearn/\n",
      "https://www.oii.ox.ac.uk/research/projects/digital-transformation-of-teaching-through-objects/\n",
      "https://www.oii.ox.ac.uk/research/projects/geonet/\n",
      "https://www.oii.ox.ac.uk/follow-us\n",
      "https://www.oii.ox.ac.uk/?page_id=482\n",
      "#############404 Error################ https://www.oii.ox.ac.uk/?page_id=482\n",
      "https://www.oii.ox.ac.uk/webcams/\n",
      "https://www.oii.ox.ac.uk/terms-of-use/\n",
      "https://www.oii.ox.ac.uk/copyright-policy/\n",
      "https://www.oii.ox.ac.uk/privacy-policy/\n",
      "https://www.oii.ox.ac.uk/cookie-statement/\n",
      "https://www.oii.ox.ac.uk/accessibility-statement/\n",
      "https://www.oii.ox.ac.uk\n",
      "https://www.oii.ox.ac.uk/research/digital-economies/\n",
      "https://www.oii.ox.ac.uk/research/information-geography-and-inequality/\n",
      "https://www.oii.ox.ac.uk/research/digital-politics-and-government/\n",
      "https://www.oii.ox.ac.uk/research/education-digital-life-and-wellbeing/\n",
      "https://www.oii.ox.ac.uk/research/ethics-and-philosophy-of-information/\n",
      "https://www.oii.ox.ac.uk/research/digital-knowledge-and-culture/\n",
      "https://www.oii.ox.ac.uk/research/information-governance-and-security/\n",
      "https://www.oii.ox.ac.uk/research/social-data-science/\n",
      "https://www.oii.ox.ac.uk/research/projects/online-populist-challenges-to-europe/\n",
      "https://www.oii.ox.ac.uk/research/past-projects/\n",
      "https://www.oii.ox.ac.uk/research/projects/a-european-ethical-code-for-data-donation/\n",
      "https://www.oii.ox.ac.uk/research/projects/a-fairwork-foundation-towards-fair-work-in-the-platform-economy/\n",
      "https://www.oii.ox.ac.uk/research/projects/child-protection/\n",
      "https://www.oii.ox.ac.uk/research/projects/computational-propaganda/\n",
      "https://www.oii.ox.ac.uk/research/projects/current-affairs-2-0-agenda-setting-in-the-european-union/\n",
      "https://www.oii.ox.ac.uk/research/projects/cybersecurity/\n",
      "https://www.oii.ox.ac.uk/research/projects/economic-geog-darknet/\n",
      "https://www.oii.ox.ac.uk/research/projects/elections-and-the-internet/\n",
      "https://www.oii.ox.ac.uk/research/projects/ethical-auditing-for-automated-decision-making/\n",
      "https://www.oii.ox.ac.uk/research/projects/gcrf-decent-work/\n",
      "https://www.oii.ox.ac.uk/research/projects/government-on-the-web/\n",
      "https://www.oii.ox.ac.uk/research/projects/ilabour/\n",
      "https://www.oii.ox.ac.uk/research/projects/internet-geographies-leverhulme-prize/\n",
      "https://www.oii.ox.ac.uk/research/projects/internet-geographies/\n",
      "https://www.oii.ox.ac.uk/research/projects/open-cabinet/\n",
      "https://www.oii.ox.ac.uk/research/projects/oxdeg/\n",
      "https://www.oii.ox.ac.uk/research/projects/oxford-martin-programme-on-misinformation-science-and-media/\n",
      "https://www.oii.ox.ac.uk/research/projects/oxford-martin-programme-on-the-illegal-wildlife-trade/\n",
      "https://www.oii.ox.ac.uk/research/projects/?page=2\n",
      "https://www.oii.ox.ac.uk/research/books/computational-propaganda-political-parties-politicians-and-political-manipulation-on-social-media/\n",
      "https://www.oii.ox.ac.uk/research/books/reinventing-capitalism-in-the-age-of-big-data/\n",
      "https://www.oii.ox.ac.uk/research/books/social-theory-after-the-internet-media-technology-and-globalization/\n",
      "https://www.oii.ox.ac.uk/research/books/web-as-history/\n",
      "https://www.oii.ox.ac.uk/news/releases/art-market-2-0-new-report-outlines-future-equitable-art-market-powered-by-blockchain-technologies/\n",
      "https://www.oii.ox.ac.uk/blog/code-like-a-girl-how-can-we-increase-the-number-of-women-in-stem/\n",
      "https://www.oii.ox.ac.uk/publications/gigwork.pdf\n",
      "http://www.oii.ox.ac.uk/publications/The_Internet_and_Business_Process_Outsourcing_in_East_Africa.pdf\n",
      "http://www.oii.ox.ac.uk/publications/Connectivity_and_the_Tea_Sector_in_Rwanda.pdf\n",
      "https://www.oii.ox.ac.uk/publications/platform-sourcing.pdf\n",
      "https://www.oii.ox.ac.uk/people?tab=visitors\n",
      "https://www.oii.ox.ac.uk/people/philip-howard/\n",
      "https://www.oii.ox.ac.uk/people/victoria-nash/\n",
      "https://www.oii.ox.ac.uk/people/andrew-przybylski/\n",
      "https://www.oii.ox.ac.uk/people/greg-taylor/\n",
      "https://www.oii.ox.ac.uk/people/mohammad-anwar/\n",
      "https://www.oii.ox.ac.uk/people/grant-blank/\n",
      "https://www.oii.ox.ac.uk/people/liliana-bounegru/\n",
      "https://www.oii.ox.ac.uk/people/fabian-braesemann/\n",
      "https://www.oii.ox.ac.uk/people/jonathan-bright/\n",
      "https://www.oii.ox.ac.uk/people/chris-burr/\n",
      "https://www.oii.ox.ac.uk/people/chico-camargo/\n",
      "https://www.oii.ox.ac.uk/people/jamie-cameron/\n",
      "https://www.oii.ox.ac.uk/people/gretta-corporaal/\n",
      "Number of pages with any of the stopwords: 40\n",
      "Number of pages without any of the stopwords: 59\n"
     ]
    }
   ],
   "source": [
    "# Part 3. Translate your pseudocode spider to working code \n",
    "\n",
    "# Here we will want to run a crawler.\n",
    "# Call your getLinks() method from within your working code. \n",
    "# I assume it will be an extension of my code in the lecture.\n",
    "#\n",
    "# Please note, you should use a set or other form of counter to ensure\n",
    "# that you do not visit the same link twice. I have warned IT about today. \n",
    "# But still...let's try not to DDOS the deparmtnet webpage. \n",
    "# Note 1.  Please exempt any page with /study See updated code snippet. \n",
    "# Note 2. Max links = 100\n",
    "# Note 3. time.sleep(0.2)\n",
    "\n",
    "################################\n",
    "# Answer below here \n",
    "\n",
    "withwordlist, withoutwordlist = spider([\"https://www.oii.ox.ac.uk/\"],[\"network\", \"networks\"],100)\n",
    "print()\n",
    "print('Number of pages with any of the stopwords:', len(withwordlist))\n",
    "print('Number of pages without any of the stopwords:', len(withoutwordlist))\n",
    "\n",
    "################################\n",
    "# Peer review comments below here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
